# -*- coding: utf-8 -*-
"""EfficientCloudOptimization

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TxZ7q__kdr3G33__hXI1NFmywqI_V3Kl

# **ARIMA Model for Predicting Resource Demand from Dataset**

**Introduction**

This report demonstrates how ARIMA modeling is implemented to predict the demand for resources in the future based on historical data regarding their utilization. This, in broader terms, aims at automating the scalability of resources in the environment of cloud computing. Such an implementation makes it possible to predict when additional resources will be needed to optimize both performance and resource efficiency.

# **Instructions**

**Mount Google Drive:**

*   Ensure that the dataset is stored in Google Drive and is accessible.
*   Use the following command to mount Google Drive:
"""

from google.colab import drive
drive.mount('/content/drive')

"""**Install Required Libraries:**


*   Install necessary Python libraries for time series analysis and model building.
"""

!pip install pmdarima
!pip install pandas numpy matplotlib statsmodels

"""**Import Libraries:**


*   Import the essential libraries for data manipulation, visualization, and model creation.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from pmdarima import auto_arima
from sklearn.metrics import mean_squared_error

"""**Load and Prepare the Dataset:**

*   Load the CSV file containing the resource utilization data from Google Drive.
*   Convert the `Timestamp` column to a datetime format, sort the data by timestamp, and set it as the index.
"""

file_path = '/content/drive/MyDrive/CloudComputing/Dataset/2xlarge.csv'
data = pd.read_csv(file_path)
data['Timestamp'] = pd.to_datetime(data['Timestamp'])
data = data.sort_values(by='Timestamp')
data.set_index('Timestamp', inplace=True)
data

"""**Visualize the Data:**

*   Plot the `Average` column to observe trends in resource utilization over time.
"""

data['Average'].plot(figsize=(12, 6), title='Average Utilization Over Time')
plt.show()

"""**Initial Analysis with ACF and PACF:**

*   Analyze the dataset using Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots to understand the series' dependencies.
"""

plot_acf(data['Average'], lags=40)
plot_pacf(data['Average'], lags=40)
plt.show()

"""**Optimal ARIMA Model Selection:**

*   Use the `auto_arima` function to automatically determine the best parameters `(p, d, q)` for the ARIMA model.
"""

auto_model = auto_arima(data['Average'],
                        start_p=1, start_q=1,
                        max_p=5, max_q=5,
                        seasonal=False,
                        trace=True,
                        error_action='ignore',
                        suppress_warnings=True,
                        stepwise=True)
print(f'Optimal ARIMA order: {auto_model.order}')

"""**Split the Dataset into Training and Test Sets:**

*   Define the training set as 60% of the data and use the remaining 40% for testing the model.
*   Plot the training and testing data
"""

train_size = int(len(data) * 0.6)
train, test = data[:train_size], data[train_size:]
train['Average'].plot(figsize=(12, 6), label='Training Set')
test['Average'].plot(figsize=(12, 6), label='Test Set')
plt.title('Training and Test Sets')
plt.legend(loc='upper left')
plt.show()

"""**Fit the ARIMA Model:**

*   Train the ARIMA model using the optimal parameters and print the summary of the fitted model.
"""

p, d, q = auto_model.order
model = ARIMA(train['Average'], order=(p, d, q))
model_fit = model.fit()
print(model_fit.summary())

"""**Model Validation:**

*   Forecast values for the test set period and compare them with the actual values to evaluate the model's performance.
"""

start = len(train)
end = len(train) + len(test) - 1
predictions = model_fit.predict(start=start, end=end, typ='levels')

plt.figure(figsize=(12, 6))
plt.plot(train.index, train['Average'], label='Train')
plt.plot(test.index, test['Average'], label='Test')
plt.plot(test.index, predictions, label='Predicted', color='red')
plt.legend(loc='upper left')
plt.title('ARIMA Model Predictions vs Actual Data')
plt.show()

mse = mean_squared_error(test['Average'], predictions)
print(f'Mean Squared Error: {mse}')

"""**Predict Future Resource Demand:**

*   Use the model to forecast future resource utilization and plot these predictions.
"""

future_steps = 200
future_predictions = model_fit.forecast(steps=future_steps)

plt.figure(figsize=(12, 6))
plt.plot(data.index, data['Average'], label='Historical Data')
future_dates = pd.date_range(data.index[-1], periods=future_steps, freq='5T')
plt.plot(future_dates, future_predictions, label='Future Predictions', color='red')
plt.title('Future Resource Demand Prediction')
plt.legend()
plt.show()

"""**Decision-Making Based on Predictions:**

*   Implement logic to determine if additional resources are needed based on the predicted values exceeding a certain threshold.
"""

threshold = 70

if any(future_predictions > threshold):
    print("Prediction indicates the need to increase storage or resources.")
else:
    print("No need to increase storage or resources.")

"""**Google Drive Link**

The dataset used in this analysis is stored in Google Drive. Ensure that the correct file path is used:
  
  [Google Drive Dataset](https://drive.google.com/drive/folders/17MJWpIU8eFqmZtXvUvj_6CeSavNfdfoV?usp=drive_link)

**Comment**

It is an effective implementation of using ARIMA for modeling and thus predicting future resource demands of cloud instances. The automated scaling decisions of the developed approach will enable optimal usage of cloud infrastructure, which may bring significant cost savings without performance degradation. Adjust the `threshold` parameter with regard to specific needs in terms of resource utilization.
"""
